{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GRU with attention: training and inference"
      ],
      "metadata": {
        "id": "CtevQobGVST0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOuZnsiHZoUT",
        "outputId": "d6885602-6fea-4d9f-aa10-5fab80f269bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First attempt - LSTM (no attention)"
      ],
      "metadata": {
        "id": "WeFEAlm1URM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5m_nbXPguET",
        "outputId": "fa48effd-58b2-49c6-9a54-3c2245127bfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 1594/1594 [07:41<00:00,  3.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.0312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 89/89 [00:16<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 4.3337\n",
            "Validation loss improved; saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 1594/1594 [07:10<00:00,  3.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 4.0052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 89/89 [00:16<00:00,  5.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.9501\n",
            "Validation loss improved; saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 1594/1594 [07:12<00:00,  3.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 3.6155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 89/89 [00:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.7822\n",
            "Validation loss improved; saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 1594/1594 [07:11<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 3.3577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 89/89 [00:15<00:00,  5.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.6851\n",
            "Validation loss improved; saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 1594/1594 [07:10<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 3.1578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 89/89 [00:16<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.6371\n",
            "Validation loss improved; saved best model to /content/drive/MyDrive/xsum/best_model.pt\n",
            "Saved final model state_dict to /content/drive/MyDrive/xsum/seq2seq_final.pt\n",
            "Saved tokenizer to /content/drive/MyDrive/xsum/tokenizer\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Train a Seq2Seq LSTM on the XSum dataset with early stopping, saving of best and final models, and tokenizer.\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# CONFIGURATION\n",
        "# ─────────────────────────────────────────────\n",
        "MAX_INPUT_LEN = 512\n",
        "MAX_TARGET_LEN = 64\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_WORKERS = 2\n",
        "PATIENCE = 2    # epochs with no improvement before stopping\n",
        "DELTA = 0.0     # minimum change to qualify as improvement\n",
        "\n",
        "# Specify your data directory\n",
        "DATA_DIR = \"/content/drive/MyDrive/xsum\"\n",
        "SAVE_DIR = DATA_DIR\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# DEVICE\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# CUSTOM DATASET CLASS\n",
        "# ─────────────────────────────────────────────\n",
        "class XSumDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.inputs = df[\"document\"].tolist()\n",
        "        self.targets = df[\"summary\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.inputs[idx]\n",
        "        tgt = self.targets[idx]\n",
        "\n",
        "        src_enc = self.tokenizer(\n",
        "            src,\n",
        "            max_length=MAX_INPUT_LEN,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tgt_enc = self.tokenizer(\n",
        "            tgt,\n",
        "            max_length=MAX_TARGET_LEN,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": src_enc[\"input_ids\"].squeeze(0),\n",
        "            \"target_ids\": tgt_enc[\"input_ids\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# SEQ2SEQ LSTM MODEL DEFINITION\n",
        "# ─────────────────────────────────────────────\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "        embedded_src = self.embedding(src)\n",
        "        _, (hidden, cell) = self.encoder(embedded_src)\n",
        "\n",
        "        embedded_trg = self.embedding(trg)\n",
        "        outputs, _ = self.decoder(embedded_trg, (hidden, cell))\n",
        "        logits = self.fc_out(outputs)\n",
        "        return logits\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# LOAD DATA AND TOKENIZER\n",
        "# ─────────────────────────────────────────────\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"xsum_train.csv\")).dropna().reset_index(drop=True)\n",
        "val_df   = pd.read_csv(os.path.join(DATA_DIR, \"xsum_val.csv\")).dropna().reset_index(drop=True)\n",
        "\n",
        "# Optionally subsample for faster iteration\n",
        "#train_df = train_df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "#val_df   = val_df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_dataset = XSumDataset(train_df, tokenizer)\n",
        "val_dataset   = XSumDataset(val_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# INITIALIZE MODEL, OPTIMIZER, CRITERION\n",
        "# ─────────────────────────────────────────────\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = Seq2SeqLSTM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# TRAINING LOOP WITH EARLY STOPPING\n",
        "# ─────────────────────────────────────────────\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
        "        src = batch[\"input_ids\"].to(DEVICE)\n",
        "        trg = batch[\"target_ids\"].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # input trg except last token, to predict next tokens\n",
        "        output = model(src, trg[:, :-1])\n",
        "        loss = criterion(\n",
        "            output.reshape(-1, vocab_size),\n",
        "            trg[:, 1:].reshape(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            src = batch[\"input_ids\"].to(DEVICE)\n",
        "            trg = batch[\"target_ids\"].to(DEVICE)\n",
        "            output = model(src, trg[:, :-1])\n",
        "            loss = criterion(\n",
        "                output.reshape(-1, vocab_size),\n",
        "                trg[:, 1:].reshape(-1)\n",
        "            )\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"\\tValidation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if avg_val_loss < best_val_loss - DELTA:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        best_path = os.path.join(SAVE_DIR, 'best_model.pt')\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"Validation loss improved; saved best model to {best_path}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "# Final Save\n",
        "final_model_path = os.path.join(SAVE_DIR, \"seq2seq_final.pt\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Saved final model state_dict to {final_model_path}\")\n",
        "tokenizer.save_pretrained(os.path.join(SAVE_DIR, 'tokenizer'))\n",
        "print(f\"Saved tokenizer to {os.path.join(SAVE_DIR, 'tokenizer')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSSOzFr36N5U"
      },
      "source": [
        "## Final TRAINING - GRU with attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7CJZHVwv1cV",
        "outputId": "79da663e-9659-40ae-d1ee-2591b9c2aa40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Training Epoch 1: 100%|██████████| 3188/3188 [35:21<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 4.9856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 4.2914\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 3188/3188 [35:18<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 4.2019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:35<00:00,  5.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.9865\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 3188/3188 [35:25<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 3.9741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.8581\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 3188/3188 [35:14<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 3.8494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.7888\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 3188/3188 [35:12<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 3.7671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.7288\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 3188/3188 [35:12<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Train Loss: 3.7063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.7016\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 3188/3188 [35:11<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Train Loss: 3.6613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.6796\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 3188/3188 [35:11<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Train Loss: 3.6250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.6677\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 3188/3188 [35:10<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Train Loss: 3.5963\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:36<00:00,  4.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.6574\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 3188/3188 [35:09<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Train Loss: 3.5723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:35<00:00,  4.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.6442\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 11: 100%|██████████| 3188/3188 [35:09<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | Train Loss: 3.5529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 177/177 [00:34<00:00,  5.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tValidation Loss: 3.6408\n",
            "Validation improved. Saved best model to /content/drive/MyDrive/xsum/best_model.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 12:  72%|███████▏  | 2295/3188 [25:19<09:56,  1.50it/s]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Seq2Seq GRU with attention on the XSum dataset.\n",
        "Includes dropout regularization.\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer\n",
        "from tqdm import tqdm\n",
        "from torch.amp import autocast\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# CONFIGURATION\n",
        "# ─────────────────────────────────────────────\n",
        "MAX_INPUT_LEN = 512\n",
        "MAX_TARGET_LEN = 64\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_WORKERS = 2\n",
        "PATIENCE = 2\n",
        "DROPOUT = 0.1\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/xsum\"\n",
        "SAVE_DIR = DATA_DIR\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# DATASET CLASS\n",
        "# ─────────────────────────────────────────────\n",
        "class XSumDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.inputs = df[\"document\"].tolist()\n",
        "        self.targets = df[\"summary\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.inputs[idx]\n",
        "        tgt = self.targets[idx]\n",
        "\n",
        "        src_enc = self.tokenizer(src, max_length=MAX_INPUT_LEN, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        tgt_enc = self.tokenizer(tgt, max_length=MAX_TARGET_LEN, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": src_enc[\"input_ids\"].squeeze(0),\n",
        "            \"target_ids\": tgt_enc[\"input_ids\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# ENCODER AND DECODER CLASSES WITH DROPOUT\n",
        "# ─────────────────────────────────────────────\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_token_id):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id)\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, num_layers=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        return outputs, hidden\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_token_id):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id)\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, num_layers=1)\n",
        "        self.attn_combine = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, target):\n",
        "        B, T = target.shape\n",
        "        embedded = self.dropout(self.embedding(target))\n",
        "        hidden = encoder_outputs[:, -1:, :].transpose(0, 1)\n",
        "        outputs = torch.zeros(B, T, self.out.out_features, device=target.device)\n",
        "        input_tok = target[:, 0]\n",
        "\n",
        "        for t in range(1, T):\n",
        "            input_emb = self.dropout(self.embedding(input_tok)).unsqueeze(1)\n",
        "            output, hidden = self.gru(input_emb, hidden)\n",
        "\n",
        "            attn_scores = torch.bmm(encoder_outputs, output.transpose(1, 2)).squeeze(2)\n",
        "            attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(1)\n",
        "            context = torch.bmm(attn_weights, encoder_outputs)\n",
        "\n",
        "            combined = torch.cat([output, context], dim=2)\n",
        "            combined = torch.tanh(self.attn_combine(combined))\n",
        "            token_logits = self.out(self.dropout(combined.squeeze(1)))\n",
        "            outputs[:, t] = token_logits\n",
        "\n",
        "            input_tok = target[:, t]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Seq2SeqGRUAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_token_id):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderRNN(vocab_size, embedding_dim, hidden_dim, pad_token_id)\n",
        "        self.decoder = AttnDecoderRNN(vocab_size, embedding_dim, hidden_dim, pad_token_id)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        encoder_outputs, _ = self.encoder(src)\n",
        "        output = self.decoder(encoder_outputs, trg)\n",
        "        return output\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# LOAD DATA AND TOKENIZER\n",
        "# ─────────────────────────────────────────────\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"xsum_train.csv\")).dropna().reset_index(drop=True)\n",
        "val_df = pd.read_csv(os.path.join(DATA_DIR, \"xsum_val.csv\")).dropna().reset_index(drop=True)\n",
        "\n",
        "train_dataset = XSumDataset(train_df, tokenizer)\n",
        "val_dataset = XSumDataset(val_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# TRAINING SETUP\n",
        "# ─────────────────────────────────────────────\n",
        "vocab_size = tokenizer.vocab_size\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "model = Seq2SeqGRUAttention(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, pad_token_id).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# TRAINING LOOP\n",
        "# ─────────────────────────────────────────────\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
        "        src = batch[\"input_ids\"].to(DEVICE)\n",
        "        trg = batch[\"target_ids\"].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(device_type=DEVICE):\n",
        "            output = model(src, trg)\n",
        "            loss = criterion(output[:, 1:].reshape(-1, vocab_size), trg[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            src = batch[\"input_ids\"].to(DEVICE)\n",
        "            trg = batch[\"target_ids\"].to(DEVICE)\n",
        "            with autocast(device_type=DEVICE):\n",
        "                output = model(src, trg)\n",
        "                loss = criterion(output[:, 1:].reshape(-1, vocab_size), trg[:, 1:].reshape(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"\\tValidation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        best_path = os.path.join(SAVE_DIR, 'best_model.pt')\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"Validation improved. Saved best model to {best_path}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# SAVE FINAL MODEL AND TOKENIZER\n",
        "# ─────────────────────────────────────────────\n",
        "final_model_path = os.path.join(SAVE_DIR, \"seq2seq_gru_attention_final.pt\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "torch.jit.save(torch.jit.script(model), os.path.join(SAVE_DIR, \"seq2seq_gru_attention_scripted.pt\"))\n",
        "tokenizer.save_pretrained(os.path.join(SAVE_DIR, 'tokenizer'))\n",
        "print(f\"Saved final model to {final_model_path} and scripted model to seq2seq_gru_attention_scripted.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBMtVA9w6Hoa"
      },
      "source": [
        "## Inference check - 5 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No attention model"
      ],
      "metadata": {
        "id": "QI7XVmGYU9gt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_WGFRAcpRn9",
        "outputId": "b0ebc255-3e10-457c-f15e-fdf8d9ba53d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ESEMPIO 1 ---\n",
            "Documento: The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - a charity to raise money for Nigerian sport. Mr Sodje, 37, is jointly charged with elder brothers Ef …\n",
            "Reference  : Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of charity fraud.\n",
            "Greedy     : a man who was stabbed to death in a crash in County Antrim has been named as the new Bishop of the MCC.\n",
            "Top-K      : sex seamer, one of the highest in a small group of men who travelled the RNLI could be revealed.\n",
            "Top-P      : has been named as Liverpool's BSC All Blacks and Andyist's hopes of match in 13 games to the group's Six Nations.\n",
            "Beam search: police have been arrested in connection with the death of a man who was stabbed to death.\n",
            "\n",
            "--- ESEMPIO 2 ---\n",
            "Documento: Voges was forced to retire hurt on 86 after suffering the injury while batting during the County Championship draw with Somerset on 4 June. Middlesex hope to have the Australian back for their T20 Bla …\n",
            "Reference  : Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his right leg.\n",
            "Greedy     : and midfielder Joey Barton has been named as the new manager of the Scottish Championship.\n",
            "Top-K      : Liverpool forward Rhys Patchell scored a hat-trick in the European Challenge Cup quarter-finals at Taunton.\n",
            "Top-P      : Glamorgan endured the 'I' top spot' 46-6, 451 because he will keep the job in Auckland on Friday.\n",
            "Beam search: have signed defender Joel Lynch on a season-long loan deal.\n",
            "\n",
            "--- ESEMPIO 3 ---\n",
            "Documento: Seven photographs taken in the Norfolk countryside by photographer Josh Olins will appear in the June edition. In her first sitting for a magazine, the duchess is seen looking relaxed and wearing casu …\n",
            "Reference  : The Duchess of Cambridge will feature on the cover of British Vogue to mark the magazine's centenary.\n",
            "Greedy     : and the world's largest indoor service in the US has been reunited with the tyres of the world's most famous buildings.\n",
            "Top-K      : will play world football champions because of the head during the election, when David Cameron was announced by the Senate.\n",
            "Top-P      : China is using bigger track up just over his 60s were destroyed in the UK in New York City.\n",
            "Beam search: has been named as the world's best-selling album of the Year.\n",
            "\n",
            "--- ESEMPIO 4 ---\n",
            "Documento: Chris Poole - known as \"moot\" online - created the site in 2003. It has gone on to be closely associated with offensive and often illegal activity, including instances where the images of child abuse  …\n",
            "Reference  : Google has hired the creator of one of the web's most notorious forums - 4chan.\n",
            "Greedy     : and the world's largest ever gingerbread in the US, the BBC's first parliamentary election.\n",
            "Top-K      : and UK will be \"delighted\" the government than any other agents before a successful test, the party has said.\n",
            "Top-P      : this year will attend a series of columns looks likely to hold the election campaign, Jeremy Corbyn's Zum has told the BBC.\n",
            "Beam search: has been named as the world's most powerful figure in the US.\n",
            "\n",
            "--- ESEMPIO 5 ---\n",
            "Documento: Four police officers were injured in the incident on Friday night. A man, aged 19, and a boy, aged 16, have been charged with six counts of aggravated vehicle taking. They are due to appear before Bel …\n",
            "Reference  : Two teenagers have been charged in connection with an incident in west Belfast in which a car collided with two police vehicles.\n",
            "Greedy     : a man who was found dead in a house in County Antrim has been named as the new Bishop of the M6.\n",
            "Top-K      : in Australia are still being blamed by the force to stop using a care home for more than 16 years.\n",
            "Top-P      : about a farmer in north London in Cyprus-west, County Fermanagh, have said was the subject of a jury described as an under-havianx national newspaper to help them contain one of their heroes.\n",
            "Beam search: police have been arrested on suspicion of murder after a man was found dead in a house.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "# 1) Debug sincrono CUDA (mettere PRIMA di import torch nei notebook)\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# 2) Device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 3) Parametri di generazione\n",
        "MAX_GEN_LEN = 64      # stesso MAX_TARGET_LEN usato in train\n",
        "K = 50\n",
        "P = 0.9\n",
        "BEAM_SIZE = 5\n",
        "\n",
        "# 4) Ricarica tokenizer e modello\n",
        "DATA_DIR = \"/content/drive/MyDrive/xsum\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(os.path.join(DATA_DIR, \"tokenizer\"))\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "# Ripeti la classe Seq2SeqLSTM esattamente com’era a training\n",
        "class Seq2SeqLSTM(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_emb = self.embedding(src)\n",
        "        _, (hidden, cell) = self.encoder(src_emb)\n",
        "        trg_emb = self.embedding(trg)\n",
        "        output, _ = self.decoder(trg_emb, (hidden, cell))\n",
        "        return self.fc_out(output)\n",
        "\n",
        "# Istanzia e carica pesi\n",
        "model = Seq2SeqLSTM(vocab_size).to(DEVICE)\n",
        "ckpt_path = os.path.join(DATA_DIR, \"seq2seq_final_NOATT.pt\")\n",
        "state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "# 5) Funzioni di sampling\n",
        "def assert_tensor_ok(tensor, name):\n",
        "    assert not torch.isnan(tensor).any(), f\"{name} contains NaN\"\n",
        "    assert not torch.isinf(tensor).any(), f\"{name} contains Inf\"\n",
        "\n",
        "def top_k_sampling(logits, k):\n",
        "    # logits: 1D tensor (vocab_size,)\n",
        "    assert_tensor_ok(logits, \"logits (top-k)\")\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    # probabilità sui top-k\n",
        "    probs = F.softmax(values, dim=-1)\n",
        "    assert_tensor_ok(probs, \"probs (top-k)\")\n",
        "    assert probs.sum() > 0, \"sum(probs_top_k)==0\"\n",
        "    choice = torch.multinomial(probs, 1)\n",
        "    return indices.gather(-1, choice)\n",
        "\n",
        "def top_p_sampling(logits, p):\n",
        "    assert_tensor_ok(logits, \"logits (top-p)\")\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    assert_tensor_ok(probs, \"probs (top-p)\")\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    # maschera fuori tutto ciò che eccede p (ma conserva il primo token)\n",
        "    mask = cumprobs > p\n",
        "    mask[..., 1:] = mask[..., :-1]\n",
        "    filtered = sorted_probs.masked_fill(mask, 0.0)\n",
        "    assert filtered.sum() > 0, \"sum(probs_top_p)==0\"\n",
        "    filtered = filtered / filtered.sum()\n",
        "    choice_idx = torch.multinomial(filtered, 1)\n",
        "    return sorted_indices.gather(-1, choice_idx)\n",
        "\n",
        "def generate_greedy(model, tokenizer, src_ids):\n",
        "    with torch.no_grad():\n",
        "        # encoder\n",
        "        src_emb = model.embedding(src_ids)\n",
        "        _, (h, c) = model.encoder(src_emb)\n",
        "\n",
        "        # inizializza prev come (batch,1)\n",
        "        prev = torch.full((src_ids.size(0), 1),\n",
        "                          tokenizer.pad_token_id,\n",
        "                          dtype=torch.long,\n",
        "                          device=src_ids.device)\n",
        "        seq = []\n",
        "\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            # embedding: batch x 1 -> batch x 1 x embed_dim\n",
        "            prev_emb = model.embedding(prev)\n",
        "            assert prev_emb.dim() == 3\n",
        "\n",
        "            out, (h, c) = model.decoder(prev_emb, (h, c))\n",
        "            logits = model.fc_out(out[:, -1, :])  # (batch, vocab)\n",
        "            nxt = logits.argmax(dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "            if nxt.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            seq.append(nxt.item())\n",
        "            prev = nxt  # already shape (batch,1)\n",
        "        return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "\n",
        "def generate_top_k(model, tokenizer, src_ids, k=K):\n",
        "    with torch.no_grad():\n",
        "        src_emb = model.embedding(src_ids)\n",
        "        _, (h, c) = model.encoder(src_emb)\n",
        "\n",
        "        prev = torch.full((src_ids.size(0), 1),\n",
        "                          tokenizer.pad_token_id,\n",
        "                          dtype=torch.long,\n",
        "                          device=src_ids.device)\n",
        "        seq = []\n",
        "\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            prev_emb = model.embedding(prev)         # (batch,1,emb)\n",
        "            out, (h, c) = model.decoder(prev_emb, (h, c))\n",
        "            logits = model.fc_out(out[:, -1, :]).squeeze(0)\n",
        "            nxt = top_k_sampling(logits, k).unsqueeze(0)  # (1,1)\n",
        "            if nxt.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "            seq.append(nxt.item())\n",
        "            prev = nxt\n",
        "        return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "\n",
        "def top_p_sampling(logits, p):\n",
        "    # logits: 1D tensor (vocab_size,)\n",
        "    # 1) trasforma in probabilità\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # 2) ordina decrescente\n",
        "    sorted_probs, sorted_inds = torch.sort(probs, descending=True)\n",
        "    # 3) cumulata\n",
        "    cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    # 4) costruisci mask senza sovrapposizioni di memoria\n",
        "    first = torch.zeros_like(cumprobs[..., :1], dtype=torch.bool)\n",
        "    rest  = cumprobs[..., :-1] > p\n",
        "    mask  = torch.cat([first, rest], dim=-1)    # shape == sorted_probs.shape\n",
        "    # 5) applica mask e rinormalizza\n",
        "    filtered = sorted_probs.masked_fill(mask, 0.0)\n",
        "    filtered = filtered / filtered.sum(dim=-1, keepdim=True)\n",
        "    # 6) campiona un indice dall’insieme filtrato\n",
        "    idx_in_sorted = torch.multinomial(filtered, 1)\n",
        "    # 7) torna l’indice originale\n",
        "    return sorted_inds.gather(-1, idx_in_sorted)\n",
        "\n",
        "def generate_top_p(model, tokenizer, src_ids, p=P):\n",
        "    \"\"\"\n",
        "    model     : il tuo Seq2SeqLSTM già in eval() e sul DEVICE\n",
        "    tokenizer : il T5Tokenizer usato in training\n",
        "    src_ids   : Tensor shape (1, seq_len) sul DEVICE\n",
        "    p         : soglia nucleus (es. 0.9)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # encoding\n",
        "        src_emb = model.embedding(src_ids)\n",
        "        _, (h, c) = model.encoder(src_emb)\n",
        "\n",
        "        # inizializza prev come (batch_size=1, 1)\n",
        "        prev = torch.full((src_ids.size(0), 1),\n",
        "                          tokenizer.pad_token_id,\n",
        "                          dtype=torch.long,\n",
        "                          device=src_ids.device)\n",
        "        seq = []\n",
        "\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            # embedding e decoding di un solo token\n",
        "            prev_emb = model.embedding(prev)          # (1,1,emb_dim)\n",
        "            out, (h, c) = model.decoder(prev_emb, (h, c))\n",
        "            logits = model.fc_out(out[:, -1, :]).squeeze(0)  # (vocab_size,)\n",
        "\n",
        "            # selezione top-p\n",
        "            nxt = top_p_sampling(logits, p).unsqueeze(0)     # (1,1)\n",
        "            if nxt.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            seq.append(nxt.item())\n",
        "            prev = nxt\n",
        "\n",
        "        return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "\n",
        "def generate_beam_search(model, tokenizer, src_ids, beam_size=BEAM_SIZE):\n",
        "    with torch.no_grad():\n",
        "        src_emb = model.embedding(src_ids)\n",
        "        _, (h0, c0) = model.encoder(src_emb)\n",
        "\n",
        "        beams = [([tokenizer.pad_token_id], h0, c0, 0.0)]\n",
        "        completed = []\n",
        "\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            new_beams = []\n",
        "            for seq_ids, h, c, score in beams:\n",
        "                # costruisci prev di shape (1,1)\n",
        "                prev = torch.tensor([[seq_ids[-1]]],\n",
        "                                    dtype=torch.long,\n",
        "                                    device=src_ids.device)\n",
        "                prev_emb = model.embedding(prev)    # (1,1,emb)\n",
        "                out, (h1, c1) = model.decoder(prev_emb, (h, c))\n",
        "                logits = model.fc_out(out[:, -1, :]).squeeze(0)\n",
        "                logps = F.log_softmax(logits, dim=-1)\n",
        "                topk_lp, topk_idx = torch.topk(logps, beam_size)\n",
        "\n",
        "                for lp, idx in zip(topk_lp.tolist(), topk_idx.tolist()):\n",
        "                    new_seq = seq_ids + [idx]\n",
        "                    new_score = score + lp\n",
        "                    if idx == tokenizer.eos_token_id:\n",
        "                        completed.append((new_seq, new_score))\n",
        "                    else:\n",
        "                        new_beams.append((new_seq, h1, c1, new_score))\n",
        "\n",
        "            beams = sorted(new_beams, key=lambda x: x[3], reverse=True)[:beam_size]\n",
        "            if not beams:\n",
        "                break\n",
        "\n",
        "        best = max(completed, key=lambda x: x[1]) if completed else beams[0]\n",
        "        # scarta il primo pad token\n",
        "        return tokenizer.decode(best[0][1:], skip_special_tokens=True)\n",
        "\n",
        "# 6) Esempio di inferenza su val_ds\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Ricarica il dataset di validazione\n",
        "import pandas as pd\n",
        "class XSumDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.inputs = df[\"document\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(self.inputs[idx],\n",
        "                             max_length=512, padding=\"max_length\",\n",
        "                             truncation=True, return_tensors=\"pt\")\n",
        "        return enc[\"input_ids\"].squeeze(0)\n",
        "\n",
        "val_df = pd.read_csv(os.path.join(DATA_DIR, \"xsum_val.csv\")).dropna()\n",
        "val_ds = XSumDataset(val_df, tokenizer)\n",
        "\n",
        "for i in range(5):\n",
        "    # Input\n",
        "    src_cpu = val_ds[i]                           # shape [seq_len]\n",
        "    src = src_cpu.unsqueeze(0).to(DEVICE)         # [1, seq_len]\n",
        "\n",
        "    # Reference summary (ground truth)\n",
        "    ref = val_df.loc[i, \"summary\"]\n",
        "\n",
        "    # Generazioni\n",
        "    out_greedy = generate_greedy(model, tokenizer, src)\n",
        "    out_topk   = generate_top_k(model, tokenizer, src)\n",
        "    out_topp   = generate_top_p(model, tokenizer, src)\n",
        "    out_beam   = generate_beam_search(model, tokenizer, src)\n",
        "\n",
        "    # Stampa\n",
        "    print(f\"\\n--- ESEMPIO {i+1} ---\")\n",
        "    print(\"Documento:\", tokenizer.decode(src_cpu, skip_special_tokens=True)[:200], \"…\")\n",
        "    print(\"Reference  :\", ref)\n",
        "    print(\"Greedy     :\", out_greedy)\n",
        "    print(\"Top-K      :\", out_topk)\n",
        "    print(\"Top-P      :\", out_topp)\n",
        "    print(\"Beam search:\", out_beam)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final GRU model with attention"
      ],
      "metadata": {
        "id": "3PB_z8RmVB48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59W6yOBlwJnT",
        "outputId": "2db61342-fe8b-4a6f-f07a-7e081eb7e7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- EXAMPLE 1 ---\n",
            "Doc:       The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - a charity to raise money for Nigerian sport. Mr Sodje, 37, is jointly charged with elder brothers Ef …\n",
            "Reference: Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of charity fraud.\n",
            "Greedy:    s have been charged with a \"serious\" disciplinary hearing into the Hillsborough disaster.\n",
            "Top-K:     fans to a taxi driver had a divorce, which led to a role on a former player by a sports tribunal for several months.\n",
            "Top-P:     footage-Fclem John Sheridan has been granted legal action against a report after playing by two members of the sports team.\n",
            "Beam:      prosecutors have been charged with raping a footballer who was beaten by a court in the United States.\n",
            "\n",
            "--- EXAMPLE 2 ---\n",
            "Doc:       Voges was forced to retire hurt on 86 after suffering the injury while batting during the County Championship draw with Somerset on 4 June. Middlesex hope to have the Australian back for their T20 Bla …\n",
            "Reference: Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his right leg.\n",
            "Greedy:    bowler Mark Footitt is to leave the county in the first half of the season after a hamstring injury.\n",
            "Top-K:     director has a hamstring injury to keep his team against the loss of three wingers which sent on a 12-month high for a third time.\n",
            "Top-P:     bowler Matt Bearn says which suffered his campaign at Gloucestershire and is expected to be mixed for the first time in five months.\n",
            "Beam:      batsman Tom Kohler has signed a new one-year contract with the county.\n",
            "\n",
            "--- EXAMPLE 3 ---\n",
            "Doc:       Seven photographs taken in the Norfolk countryside by photographer Josh Olins will appear in the June edition. In her first sitting for a magazine, the duchess is seen looking relaxed and wearing casu …\n",
            "Reference: The Duchess of Cambridge will feature on the cover of British Vogue to mark the magazine's centenary.\n",
            "Greedy:    - a shopper in the UK have been crowned a 164 of a shopper.\n",
            "Top-K:     wolf star and US coins a 164 coins were born in a gold park from a pair of-seater books, a ten-year-old woman has revealed.\n",
            "Top-P:     book giant panda is thought to be launching a second-round bell-par platform - by women because of a hat-tonne known as an identity papers.\n",
            "Beam:      curators have been commended for the first time.\n",
            "\n",
            "--- EXAMPLE 4 ---\n",
            "Doc:       Chris Poole - known as \"moot\" online - created the site in 2003. It has gone on to be closely associated with offensive and often illegal activity, including instances where the images of child abuse  …\n",
            "Reference: Google has hired the creator of one of the web's most notorious forums - 4chan.\n",
            "Greedy:    a video of the internet is a \"serious\" sacked a video of the internet, a US court has ruled.\n",
            "Top-K:     users should be asked to show an 'just el-hunao' school after it received more than $1bn (£395,000) to a digital camera of an internet-wide chat, regulators has said.\n",
            "Top-P:     s in the web marketplace at the age of 51 film videos, social network internet, The Hobbita Taade and newspaper mocked books has been used in the watchdog.\n",
            "Beam:      spies has been sacked for a video of a video of a video of a bribery site.\n",
            "\n",
            "--- EXAMPLE 5 ---\n",
            "Doc:       Four police officers were injured in the incident on Friday night. A man, aged 19, and a boy, aged 16, have been charged with six counts of aggravated vehicle taking. They are due to appear before Bel …\n",
            "Reference: Two teenagers have been charged in connection with an incident in west Belfast in which a car collided with two police vehicles.\n",
            "Greedy:    police have been charged with causing a car in a car park in Bedfordshire, police have said.\n",
            "Top-K:     agents armed allegedly on his car at a Belfast primary game against police were injured in a car crash.\n",
            "Top-P:     gangs have been charged with possessing a lorry crash by a man in Stirling.\n",
            "Beam:      drivers have been charged with attempted murder after a man was stabbed in a car park in Bedfordshire.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Inference for Seq2Seq GRU+Attention model on XSum.\n",
        "Supports greedy, top-k, top-p, and beam search decoding.\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# CONFIG\n",
        "# ─────────────────────────────────────────────\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_DIR    = \"/content/drive/MyDrive/xsum\"\n",
        "MAX_GEN_LEN = 64\n",
        "K           = 50\n",
        "P           = 0.9\n",
        "BEAM_SIZE   = 5\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# TOKENIZER\n",
        "# ─────────────────────────────────────────────\n",
        "tokenizer    = T5Tokenizer.from_pretrained(os.path.join(DATA_DIR, \"tokenizer\"))\n",
        "VOCAB_SIZE   = tokenizer.vocab_size\n",
        "PAD_ID       = tokenizer.pad_token_id\n",
        "EOS_ID       = tokenizer.eos_token_id\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# MODEL DEFINITION\n",
        "# ─────────────────────────────────────────────\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM    = 512\n",
        "DROPOUT       = 0.1\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, pad_id):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
        "        self.dropout   = nn.Dropout(DROPOUT)\n",
        "        self.gru       = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb      = self.dropout(self.embedding(x))\n",
        "        outputs, hidden = self.gru(emb)\n",
        "        return outputs, hidden\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, pad_id):\n",
        "        super().__init__()\n",
        "        self.embedding    = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
        "        self.dropout      = nn.Dropout(DROPOUT)\n",
        "        self.gru          = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
        "        self.attn_combine = nn.Linear(hid_dim*2, hid_dim)\n",
        "        self.out          = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def step(self, input_tok, hidden, enc_outputs):\n",
        "        emb      = self.dropout(self.embedding(input_tok)).unsqueeze(1)      # (b,1,emb)\n",
        "        output, hidden = self.gru(emb, hidden)                               # (b,1,hid)\n",
        "        attn_scores    = torch.bmm(enc_outputs, output.transpose(1,2)).squeeze(2)  # (b,seq)\n",
        "        attn_weights   = torch.softmax(attn_scores, dim=1).unsqueeze(1)      # (b,1,seq)\n",
        "        context        = torch.bmm(attn_weights, enc_outputs)                # (b,1,hid)\n",
        "        combined       = torch.cat([output, context], dim=2)                 # (b,1,2*hid)\n",
        "        combined       = torch.tanh(self.attn_combine(combined))             # (b,1,hid)\n",
        "        logits         = self.out(self.dropout(combined.squeeze(1)))         # (b,vocab)\n",
        "        return logits, hidden\n",
        "\n",
        "class Seq2SeqGRUAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, pad_id):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderRNN(vocab_size, emb_dim, hid_dim, pad_id)\n",
        "        self.decoder = AttnDecoderRNN(vocab_size, emb_dim, hid_dim, pad_id)\n",
        "\n",
        "    def encode(self, src):\n",
        "        return self.encoder(src)\n",
        "\n",
        "    def decode_step(self, input_tok, hidden, enc_outputs):\n",
        "        return self.decoder.step(input_tok, hidden, enc_outputs)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# LOAD MODEL\n",
        "# ─────────────────────────────────────────────\n",
        "model = Seq2SeqGRUAttention(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, PAD_ID).to(DEVICE)\n",
        "state = torch.load(os.path.join(DATA_DIR, \"seq2seq_gru_attention_final.pt\"), map_location=DEVICE)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# SAMPLING HELPERS\n",
        "# ─────────────────────────────────────────────\n",
        "def top_k_sampling(logits, k):\n",
        "    vals, idxs = torch.topk(F.softmax(logits, dim=-1), k)\n",
        "    vals       = vals / vals.sum(dim=-1, keepdim=True)\n",
        "    choice     = torch.multinomial(vals, 1)\n",
        "    return idxs.gather(-1, choice)\n",
        "\n",
        "def top_p_sampling(logits, p, eps=1e-8):\n",
        "    # 1) converti in probabilità\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # 2) ordina decrescente\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    # 3) cumulata\n",
        "    cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    # 4) crea mask shifted senza aliasing\n",
        "    orig_mask = cumprobs > p\n",
        "    mask = torch.zeros_like(orig_mask)\n",
        "    mask[..., 1:] = orig_mask[..., :-1]\n",
        "    # 5) applica mask e rinormalizza\n",
        "    filtered = sorted_probs.masked_fill(mask, 0.0)\n",
        "    if filtered.sum() < eps:\n",
        "        filtered = torch.ones_like(filtered)\n",
        "        filtered[:10] = 1\n",
        "    filtered = filtered / filtered.sum()\n",
        "    # 6) campiona\n",
        "    idx_in_sorted = torch.multinomial(filtered, 1)\n",
        "    # 7) mappa indici al vocab\n",
        "    return sorted_indices.gather(-1, idx_in_sorted)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# GENERATION FUNCTIONS\n",
        "# ─────────────────────────────────────────────\n",
        "def generate(model, tokenizer, src_ids, mode=\"greedy\"):\n",
        "    with torch.no_grad():\n",
        "        enc_outs, hidden = model.encode(src_ids)\n",
        "        input_tok = torch.full((src_ids.size(0),), PAD_ID, dtype=torch.long, device=DEVICE)\n",
        "        seq = []\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            logits, hidden = model.decode_step(input_tok, hidden, enc_outs)\n",
        "            if mode == \"greedy\":\n",
        "                next_tok = logits.argmax(dim=-1)\n",
        "            elif mode == \"top_k\":\n",
        "                next_tok = top_k_sampling(logits, K).squeeze(-1)\n",
        "            elif mode == \"top_p\":\n",
        "                next_tok = top_p_sampling(logits, P).squeeze(-1)\n",
        "            else:\n",
        "                raise ValueError(\"Unknown mode\")\n",
        "            if next_tok.item() == EOS_ID:\n",
        "                break\n",
        "            seq.append(next_tok.item())\n",
        "            input_tok = next_tok\n",
        "        return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "\n",
        "def generate_beam_search(model, tokenizer, src_ids, beam_size=BEAM_SIZE):\n",
        "    with torch.no_grad():\n",
        "        enc_outs, hidden = model.encode(src_ids)\n",
        "        beams = [([PAD_ID], hidden, 0.0)]\n",
        "        completed = []\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            new_beams = []\n",
        "            for seq_ids, h, score in beams:\n",
        "                prev = torch.tensor([seq_ids[-1]], device=DEVICE).unsqueeze(0)\n",
        "                logits, h1 = model.decode_step(prev.squeeze(0), h, enc_outs)\n",
        "                logps = F.log_softmax(logits, dim=-1)\n",
        "                top_lp, top_idx = torch.topk(logps, beam_size, dim=-1)\n",
        "                for lp, idx in zip(top_lp[0], top_idx[0]):\n",
        "                    new_seq = seq_ids + [idx.item()]\n",
        "                    new_score = score + lp.item()\n",
        "                    if idx.item() == EOS_ID:\n",
        "                        completed.append((new_seq, new_score))\n",
        "                    else:\n",
        "                        new_beams.append((new_seq, h1, new_score))\n",
        "            beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:beam_size]\n",
        "            if not beams:\n",
        "                break\n",
        "        best = max(completed, key=lambda x: x[1]) if completed else beams[0]\n",
        "        return tokenizer.decode(best[0][1:], skip_special_tokens=True)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# RUN EXAMPLES\n",
        "# ─────────────────────────────────────────────\n",
        "class XSumDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.inputs = df[\"document\"].tolist()\n",
        "        self.tok    = tokenizer\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tok(self.inputs[i],\n",
        "                       max_length=512,\n",
        "                       padding=\"max_length\",\n",
        "                       truncation=True,\n",
        "                       return_tensors=\"pt\")\n",
        "        return enc[\"input_ids\"].squeeze(0)\n",
        "\n",
        "val_df = pd.read_csv(os.path.join(DATA_DIR, \"xsum_val.csv\")).dropna().reset_index(drop=True)\n",
        "val_ds = XSumDataset(val_df, tokenizer)\n",
        "\n",
        "for i in range(5):\n",
        "    src = val_ds[i].unsqueeze(0).to(DEVICE)\n",
        "    ref = val_df.loc[i, \"summary\"]\n",
        "    print(f\"\\n--- EXAMPLE {i+1} ---\")\n",
        "    print(\"Doc:      \", tokenizer.decode(src[0], skip_special_tokens=True)[:200], \"…\")\n",
        "    print(\"Reference:\", ref)\n",
        "    print(\"Greedy:   \", generate(model, tokenizer, src, mode=\"greedy\"))\n",
        "    print(\"Top-K:    \", generate(model, tokenizer, src, mode=\"top_k\"))\n",
        "    print(\"Top-P:    \", generate(model, tokenizer, src, mode=\"top_p\"))\n",
        "    print(\"Beam:     \", generate_beam_search(model, tokenizer, src))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final INFERENCE on all test set (GRU with attention)"
      ],
      "metadata": {
        "id": "TA09HY25U1ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different decoding strategies are implemented."
      ],
      "metadata": {
        "id": "unDicCuaVONW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEb3uodJcm1K"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Fast, batched inference for Seq2Seq GRU+Attention on the full XSum test set.\n",
        "Generates summaries via greedy, top-k, top-p, and beam search in batch.\n",
        "Supports temperature scaling, early EOS stopping, and vectorized repetition penalty.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# CONFIGURATION\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "DEVICE              = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_DIR            = \"/content/drive/MyDrive/xsum\"\n",
        "MAX_GEN_LEN         = 64\n",
        "K                    = 50\n",
        "P                    = 0.9\n",
        "BEAM_SIZE            = 4\n",
        "BATCH_SIZE           = 64\n",
        "NUM_WORKERS          = 2\n",
        "TEMPERATURE          = 1.0\n",
        "REPETITION_PENALTY  = 1.2   # penalty factor >1\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# TOKENIZER & SPECIAL IDS\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "tokenizer = T5Tokenizer.from_pretrained(os.path.join(DATA_DIR, \"tokenizer\"))\n",
        "PAD_ID     = tokenizer.pad_token_id\n",
        "EOS_ID     = tokenizer.eos_token_id\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# MODEL DEFINITION\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "EMBED_DIM  = 256\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT    = 0.1\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=PAD_ID)\n",
        "        self.drop  = nn.Dropout(DROPOUT)\n",
        "        self.gru   = nn.GRU(EMBED_DIM, HIDDEN_DIM, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        emb = self.drop(self.embed(x))\n",
        "        outputs, hidden = self.gru(emb)\n",
        "        return outputs, hidden\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embed        = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=PAD_ID)\n",
        "        self.drop         = nn.Dropout(DROPOUT)\n",
        "        self.gru          = nn.GRU(EMBED_DIM, HIDDEN_DIM, batch_first=True)\n",
        "        self.attn_combine = nn.Linear(HIDDEN_DIM*2, HIDDEN_DIM)\n",
        "        self.out          = nn.Linear(HIDDEN_DIM, vocab_size)\n",
        "    def step(self, input_tok, hidden, enc_outs):\n",
        "        emb    = self.drop(self.embed(input_tok)).unsqueeze(1)\n",
        "        output, hidden = self.gru(emb, hidden)\n",
        "        scores  = torch.bmm(enc_outs, output.transpose(1,2)).squeeze(2)\n",
        "        weights = torch.softmax(scores, dim=1).unsqueeze(1)\n",
        "        context = torch.bmm(weights, enc_outs)\n",
        "        comb    = torch.tanh(self.attn_combine(torch.cat([output, context], dim=2)))\n",
        "        logits  = self.out(self.drop(comb.squeeze(1)))\n",
        "        return logits, hidden\n",
        "\n",
        "class Seq2SeqGRUAttention(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderRNN(vocab_size)\n",
        "        self.decoder = AttnDecoderRNN(vocab_size)\n",
        "    def encode(self, src):\n",
        "        return self.encoder(src)\n",
        "    def decode_step(self, tok, hidden, enc):\n",
        "        return self.decoder.step(tok, hidden, enc)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# LOAD MODEL WITH KEY MAPPING\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "model      = Seq2SeqGRUAttention(VOCAB_SIZE).to(DEVICE)\n",
        "orig_state = torch.load(os.path.join(DATA_DIR, \"best_model.pt\"), map_location=DEVICE)\n",
        "state = {}\n",
        "for k,v in orig_state.items():\n",
        "    if k.startswith(\"encoder.embedding.\"):\n",
        "        new_k = k.replace(\"encoder.embedding.\", \"encoder.embed.\")\n",
        "    elif k.startswith(\"decoder.embedding.\"):\n",
        "        new_k = k.replace(\"decoder.embedding.\", \"decoder.embed.\")\n",
        "    else:\n",
        "        new_k = k\n",
        "    state[new_k] = v\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# BATCHED GENERATION FUNCTIONS WITH REPETITION PENALTY\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def generate_greedy_batch(src_ids, temperature=TEMPERATURE):\n",
        "    B = src_ids.size(0)\n",
        "    enc_outs, hidden = model.encode(src_ids)\n",
        "    prev = torch.full((B,), PAD_ID, device=DEVICE)\n",
        "    seqs = [[] for _ in range(B)]\n",
        "    used  = torch.zeros(B, VOCAB_SIZE, device=DEVICE)\n",
        "    finished = torch.zeros(B, dtype=torch.bool, device=DEVICE)\n",
        "\n",
        "    with autocast(device_type=DEVICE), torch.no_grad():\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            logits, hidden = model.decode_step(prev, hidden, enc_outs)\n",
        "            scaled = logits / temperature\n",
        "            # apply repetition penalty: lower logits of used tokens\n",
        "            penalty_mask = torch.where(used>0, REPETITION_PENALTY, 1.0)\n",
        "            scaled = scaled / penalty_mask\n",
        "            # mask finished sequences\n",
        "            scaled[finished] = float('-inf'); scaled[finished, EOS_ID] = 0\n",
        "\n",
        "            next_tok = scaled.argmax(dim=-1)\n",
        "            for i, tok in enumerate(next_tok.tolist()):\n",
        "                if not finished[i]:\n",
        "                    seqs[i].append(tok)\n",
        "                    used[i, tok] = 1\n",
        "                    if tok == EOS_ID:\n",
        "                        finished[i] = True\n",
        "            prev = next_tok\n",
        "            if finished.all(): break\n",
        "\n",
        "    return [tokenizer.decode(seq, skip_special_tokens=True) for seq in seqs]\n",
        "\n",
        "\n",
        "def generate_top_k_batch(src_ids, k=K, temperature=TEMPERATURE):\n",
        "    B = src_ids.size(0)\n",
        "    enc_outs, hidden = model.encode(src_ids)\n",
        "    prev = torch.full((B,), PAD_ID, device=DEVICE)\n",
        "    seqs = [[] for _ in range(B)]\n",
        "    used  = torch.zeros(B, VOCAB_SIZE, device=DEVICE)\n",
        "    finished = torch.zeros(B, dtype=torch.bool, device=DEVICE)\n",
        "\n",
        "    with autocast(device_type=DEVICE), torch.no_grad():\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            logits, hidden = model.decode_step(prev, hidden, enc_outs)\n",
        "            scaled = logits / temperature\n",
        "            penalty_mask = torch.where(used>0, REPETITION_PENALTY, 1.0)\n",
        "            scaled = scaled / penalty_mask\n",
        "            scaled[finished] = float('-inf'); scaled[finished, EOS_ID] = 0\n",
        "\n",
        "            vals, idxs = torch.topk(F.softmax(scaled, dim=-1), k, dim=-1)\n",
        "            probs = vals / vals.sum(dim=-1, keepdim=True)\n",
        "            choice = torch.multinomial(probs, 1).squeeze(-1)\n",
        "            next_tok = idxs.gather(-1, choice.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            for i, tok in enumerate(next_tok.tolist()):\n",
        "                if not finished[i]:\n",
        "                    seqs[i].append(tok)\n",
        "                    used[i, tok] = 1\n",
        "                    if tok == EOS_ID:\n",
        "                        finished[i] = True\n",
        "            prev = next_tok\n",
        "            if finished.all(): break\n",
        "\n",
        "    return [tokenizer.decode(seq, skip_special_tokens=True) for seq in seqs]\n",
        "\n",
        "\n",
        "def generate_top_p_batch(src_ids, p=P, temperature=TEMPERATURE):\n",
        "    B = src_ids.size(0)\n",
        "    enc_outs, hidden = model.encode(src_ids)\n",
        "    prev = torch.full((B,), PAD_ID, device=DEVICE)\n",
        "    seqs = [[] for _ in range(B)]\n",
        "    used  = torch.zeros(B, VOCAB_SIZE, device=DEVICE)\n",
        "    finished = torch.zeros(B, dtype=torch.bool, device=DEVICE)\n",
        "\n",
        "    with autocast(device_type=DEVICE), torch.no_grad():\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            logits, hidden = model.decode_step(prev, hidden, enc_outs)\n",
        "            scaled = logits / temperature\n",
        "            penalty_mask = torch.where(used>0, REPETITION_PENALTY, 1.0)\n",
        "            scaled = scaled / penalty_mask\n",
        "            scaled[finished] = float('-inf'); scaled[finished, EOS_ID] = 0\n",
        "\n",
        "            probs = F.softmax(scaled, dim=-1)\n",
        "            sorted_p, sorted_i = torch.sort(probs, descending=True)\n",
        "            cum = sorted_p.cumsum(dim=-1)\n",
        "            mask = cum > p; mask[...,1:] = mask[..., :-1]; mask[...,0] = False\n",
        "            filt = sorted_p.masked_fill(mask, 0.0)\n",
        "            if filt.sum(dim=-1, keepdim=True).eq(0).any(): filt = torch.ones_like(filt)\n",
        "            filt = filt / filt.sum(dim=-1, keepdim=True)\n",
        "            choice = torch.multinomial(filt, 1).squeeze(-1)\n",
        "            next_tok = sorted_i.gather(-1, choice.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            for i, tok in enumerate(next_tok.tolist()):\n",
        "                if not finished[i]:\n",
        "                    seqs[i].append(tok)\n",
        "                    used[i, tok] = 1\n",
        "                    if tok == EOS_ID:\n",
        "                        finished[i] = True\n",
        "            prev = next_tok\n",
        "            if finished.all(): break\n",
        "\n",
        "    return [tokenizer.decode(seq, skip_special_tokens=True) for seq in seqs]\n",
        "\n",
        "\n",
        "def generate_beam_batch(src_ids, beam_size=BEAM_SIZE, temperature=TEMPERATURE):\n",
        "    B = src_ids.size(0)\n",
        "    enc_outs, hidden = model.encode(src_ids)\n",
        "    enc = enc_outs.unsqueeze(1).expand(B, beam_size, -1, -1).reshape(B*beam_size, -1, HIDDEN_DIM)\n",
        "    h   = hidden.unsqueeze(2).expand(-1, B, beam_size, -1).reshape(1, B*beam_size, -1)\n",
        "    beams  = torch.full((B*beam_size,1), PAD_ID, device=DEVICE)\n",
        "    scores = torch.zeros(B, beam_size, device=DEVICE)\n",
        "    used   = torch.zeros(B*beam_size, VOCAB_SIZE, device=DEVICE)\n",
        "    finished = torch.zeros(B*beam_size, dtype=torch.bool, device=DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(MAX_GEN_LEN):\n",
        "            prev = beams[:, -1]\n",
        "            logits, h = model.decode_step(prev, h, enc)\n",
        "            scaled = logits / temperature\n",
        "            penalty_mask = torch.where(used>0, REPETITION_PENALTY, 1.0)\n",
        "            scaled = scaled / penalty_mask\n",
        "            scaled[finished] = float('-inf'); scaled[finished, EOS_ID] = 0\n",
        "\n",
        "            logp = F.log_softmax(scaled, dim=-1).view(B, beam_size, -1)\n",
        "            total = logp + scores.unsqueeze(-1)\n",
        "            flat_scores, flat_idx = total.view(B, -1).topk(beam_size, dim=-1)\n",
        "            beam_idx = flat_idx // VOCAB_SIZE; tok_idx = flat_idx % VOCAB_SIZE\n",
        "\n",
        "            old = beams.view(B, beam_size, -1)\n",
        "            new_beams, new_used, new_finished, new_scores = [], [], [], []\n",
        "            for b in range(B):\n",
        "                for i in range(beam_size):\n",
        "                    bi = beam_idx[b,i]; ti = tok_idx[b,i]\n",
        "                    seq = torch.cat([old[b,bi], ti.view(1)])\n",
        "                    new_beams.append(seq)\n",
        "                    idx_flat = b*beam_size + i\n",
        "                    # update used\n",
        "                    u = used[idx_flat].clone()\n",
        "                    u[ti] = 1\n",
        "                    new_used.append(u)\n",
        "                    # update finished and score\n",
        "                    fin = finished[idx_flat] or (ti==EOS_ID)\n",
        "                    new_finished.append(fin)\n",
        "                    new_scores.append(flat_scores[b,i])\n",
        "            beams = torch.stack(new_beams).view(B*beam_size, -1)\n",
        "            used = torch.stack(new_used)\n",
        "            finished = torch.tensor(new_finished, device=DEVICE)\n",
        "            scores = torch.stack(new_scores).view(B, beam_size)\n",
        "            # reorder hidden\n",
        "            h = h.view(1, B, beam_size, HIDDEN_DIM)\n",
        "            h = h.gather(2, beam_idx.view(1,B,beam_size,1).expand(-1,-1,-1,HIDDEN_DIM))\n",
        "            h = h.reshape(1, B*beam_size, HIDDEN_DIM)\n",
        "            if finished.all(): break\n",
        "\n",
        "    results = []\n",
        "    beams = beams.view(B, beam_size, -1)\n",
        "    best  = scores.argmax(dim=-1)\n",
        "    for b in range(B):\n",
        "        seq = beams[b, best[b],1:].tolist()\n",
        "        if EOS_ID in seq: seq = seq[:seq.index(EOS_ID)]\n",
        "        results.append(tokenizer.decode(seq, skip_special_tokens=True))\n",
        "    return results\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# DATASET & RUN\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "class XSumTestDataset(Dataset):\n",
        "    def __init__(self, path): self.df = pd.read_csv(path).dropna().reset_index(drop=True)\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        enc = tokenizer(self.df.loc[i,\"document\"], max_length=512,\n",
        "                        padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        return enc[\"input_ids\"].squeeze(0)\n",
        "\n",
        "csv_in = os.path.join(DATA_DIR, \"xsum_test.csv\")\n",
        "loader = DataLoader(XSumTestDataset(csv_in), batch_size=BATCH_SIZE,\n",
        "                    shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "pred_g, pred_k, pred_p, pred_b = [], [], [], []\n",
        "for batch in tqdm(loader, desc=\"Greedy\"):   pred_g += generate_greedy_batch(batch.to(DEVICE))\n",
        "for batch in tqdm(loader, desc=\"Top-K\"):    pred_k += generate_top_k_batch(batch.to(DEVICE))\n",
        "for batch in tqdm(loader, desc=\"Top-P\"):    pred_p += generate_top_p_batch(batch.to(DEVICE))\n",
        "for batch in tqdm(loader, desc=\"Beam\"):     pred_b += generate_beam_batch(batch.to(DEVICE))\n",
        "\n",
        "out_df = pd.read_csv(csv_in).dropna().reset_index(drop=True)\n",
        "out_df[\"pred_greedy\"] = pred_g\n",
        "out_df[\"pred_top_k\"]   = pred_k\n",
        "out_df[\"pred_top_p\"]   = pred_p\n",
        "out_df[\"pred_beam\"]    = pred_b\n",
        "out_df.to_csv(os.path.join(DATA_DIR, \"xsum_test_with_preds.csv\"), index=False)\n",
        "print(\"Saved to xsum_test_with_preds.csv\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "WeFEAlm1URM7",
        "qSSOzFr36N5U",
        "oBMtVA9w6Hoa",
        "QI7XVmGYU9gt",
        "3PB_z8RmVB48",
        "TA09HY25U1ay"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}